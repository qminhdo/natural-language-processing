Từ Hán cổ và từ Hán Việt được gọi chung là từ gốc Hán.

Một số từ ngữ Hán cổ có thể kể đến như "đầu", "gan", "ghế", "ông", "bà", "cô", "chè", "ngà", "chén", "chém", "chìm", "buồng", "buồn", "buồm", "mùi", "mùa"... Từ Hán cổ là những từ gốc Hán được du nhập vào tiếng Việt đã lâu, đã được đồng hoá rất mạnh, nên những từ này hiện nay là từ thông thường trong hoạt động xã hội đối với người Việt.

Hệ thống từ Hán Việt trong tiếng Việt bằng cách đọc các chữ Hán theo ngữ âm hiện có của tiếng Việt (tương tự như người Nhật Bản áp dụng kanji đối với chữ Hán và katakana với các tiếng nước ngoài khác). Hiện nay có 1945 chữ Hán thông dụng trong tiếng Nhật, cũng có khoảng 2000 từ Hán–Hàn thông dụng). Số lượng từ vựng tiếng Việt có thêm hàng loạt các yếu tố Hán–Việt. Như là "chủ", "ở", "tâm", "minh", "đức", "thiên", "tự do",... giữ nguyên nghĩa chỉ khác cách đọc; hay thay đổi vị trí như "nhiệt náo" thành "náo nhiệt", "thích phóng" thành "phóng thích", "đảm bảo" thành "bảo đảm"...; hoặc được rút gọn như "thừa trần" thành "trần" (trong trần nhà), "lạc hoa sinh" thành "lạc" (trong củ lạc, còn gọi là đậu phộng)...; hoặc được đọc chệch đi như "tiếp thu" thành "tiếp thụ", "tháp nhập" thành "sáp nhập", "thống kế" thành "thống kê", "chúng cư" thành "chung cư", "vãn cảnh" thành "vãng cảnh", "khuyến mãi" thành "khuyến mại"...; hay đổi khác nghĩa hoàn toàn như "phương phi" trong tiếng Hán có nghĩa là "hoa cỏ thơm tho" thì trong tiếng Việt lại là "béo tốt", "bồi hồi" trong tiếng Hán nghĩa là "đi đi lại lại" sang tiếng Việt thành "bồn chồn, xúc động"... Mặt khác, người Trung Quốc gọi là Thái Sơn, Hoàng Hà, cổ thụ... thì người Việt lại đọc là núi Thái Sơn, sông Hoàng Hà, cây cổ thụ (mặc dù sơn = núi, hà = sông, thụ = cây)... Do tính quy ước của ngôn ngữ mà ít nhiều các cách đọc sai khác với tiếng Hán vẫn được chấp nhận và sử dụng rộng rãi, trong khi các nhà nghiên cứu ngôn ngữ tiếng Việt hiện nay cũng như các cơ quan, các cấp quản lý, tổ chức xã hội – nghề nghiệp lẫn các nhà khoa học Việt Nam chưa tìm được tiếng nói chung trong việc chuẩn hoá cách sử dụng tên riêng và từ vựng mượn từ tiếng nước ngoài.[13]. Bên cạnh đó, cũng có những từ được cho là dùng sai và khó chấp nhận như, “quan ngại” được dùng và hiểu như “lo ngại”, “vấn nạn” được hiểu là “vấn đề nan giải”, “vô hình trung” thì viết thành “vô hình chung” hay “vô hình dung”, “việt dã” bị hiểu là “chạy dài”; “trứ tác” được dùng như “sáng tác”[14], “phong thanh” được dùng như “phong phanh", “bàng quan” được dùng như “bàng quang”, “đào ngũ” được dùng là “đảo ngũ”, "tham quan" thành "thăm quan", "xán lạn" thành "sáng lạng"…

Tỉ lệ từ Hán Việt trong tiếng Việt rất lớn. Theo ước lượng của các nhà nghiên cứu, từ Hán Việt chiếm khoảng trên dưới 70% vốn từ trong phong cách chính luận, khoa học (Maspéro thì cho rằng, chúng chiếm tới hơn 60% lượng từ tiếng Việt)[13][14]. Tác giả Lê Nguyễn Lưu trong cuốn sách Từ chữ Hán đến chữ Nôm thì cho rằng, về lĩnh vực chuyên môn và khoa học tỉ lệ này có thể lên đến 80%, nhưng khi nhận xét về văn ngữ trong một cuốn tiểu thuyết thì chỉ còn 12,8%, kịch nói rút xuống còn 8,9%, và ngôn ngữ nói chuyện hằng ngày còn thấp hơn nữa.[15].

Các từ và từ tố Hán Việt được sử dụng để tạo ra các từ ngữ mới cho tiếng Việt như sĩ diện, phi công, bao gồm, sống động, sinh đẻ, vân vân. Trong khi tiếng Việt gọi là phát thanh (發聲) thì tiếng Hán lại gọi là 廣播 quảng bá; tiếng Việt gọi là truyền hình (傳形) thì tiếng Hán gọi là 電視 điện thị; tiếng Việt gọi là thành phố (城鋪), thị xã (市社) thì tiếng Hán gọi là 市 thị. Tiếng Việt đã lợi dụng được những thành tựu ngôn ngữ trong tiếng Hán để tự cải tiến mình.

Kể từ đầu thế kỷ thứ XI, Nho học phát triển, việc học văn tự chữ Nho được đẩy mạnh, tầng lớp trí thức được mở rộng tạo tiền đề cho một nền văn chương của người Việt bằng chữ Nho cực kỳ phát triển với các áng văn thư nổi tiếng như Nam quốc sơn hà bên sông Như Nguyệt (sông Cầu).

Cùng thời gian này, một hệ thống chữ viết được xây dựng riêng cho người Việt theo nguyên tắc ghi âm tiết được phát triển, và đó chính là chữ Nôm. Để tiện cho việc học chữ Hán và chữ Nôm của người Việt, Ngô Thì Nhậm (1746–1803) đã biên soạn cuốn sách Tam thiên tự giải âm (còn gọi là Tam thiên tự, Tự học toản yếu). Tam thiên tự giải âm chỉ lược dạy 3000 chữ Hán, Nôm thông thường, đáp ứng nhu cầu cần thiết, nhớ chữ, nhớ nghĩa từng chữ, mỗi câu bốn chữ. Hiệp vần cũng có điểm đặc biệt, tức là vần lưng (yêu vận, vần giữa câu). Tiếng thứ tư câu đầu hiệp với tiếng thứ hai câu dưới, rồi cứ thế mãi đến 3000 chữ, 750 câu. Ví dụ: Thiên – trời, địa – đất, cử – cất, tồn – còn, tử – con, tôn – cháu, lục – sáu, tam – ba, gia – nhà, quốc – nước, tiền – trước, hậu – sau, ngưu – trâu, mã – ngựa, cự – cựa, nha – răng, vô – chăng, hữu – có, khuyển – chó, dương – dê,... Trần Văn Giáp đánh giá đây tuy chỉ là quyển sách dạy học vỡ lòng về chữ Hán, như đã nêu ở trên, nhưng thực ra cũng có thể coi nó chính là sách Từ điển Hán Việt thông thường và phổ biến ở cuối thế kỷ XVIII, cùng thời với các sách Chỉ nam ngọc âm, Chỉ nam bị loại, và xuất hiện trước các sách Nhật dụng thường đàm, Thiên tự văn và Đại Nam quốc ngữ.[16] Nhờ có chữ Nôm, văn học Việt Nam đã có những bước phát triển rực rỡ nhất, đạt đỉnh cao với Truyện Kiều của Nguyễn Du (1765–1820). Tiếng Việt, được thể hiện bằng chữ Nôm ở những thời kỳ sau này về cơ bản rất gần với tiếng Việt ngày nay. Tuy hầu hết mọi người Việt đều có thể nghe và hiểu văn bản bằng chữ Nôm, chỉ những người có học chữ Nôm mới có thể đọc và viết được chữ Nôm.


Originally Answered: what is perplexity in NLP?
In English, the word 'perplexed' means 'puzzled' or 'confused' (source). Perplexity means inability to deal with or understand something complicated or unaccountable.

When a toddler or a baby speaks unintelligibly, we find ourselves 'perplexed'. Why ? because their spoken language does not comply with the grammar and construct of the language that we tend to understand and speak.

Now imagine you trained a machine learning NLP model on lots and lots of well written blogs and answers. Now the task is to evaluate how good a certain Quora answer is (for, say, pushing it to the top of the feed). Among very many models that you trained, which model will you select for picking good blogs from bad ?

Answer: You would pick that NLP model which is least "perplexed" when presented with a well written blog.

Thus, perplexity metric in NLP is a way to capture the degree of 'uncertainty' a model has in predicting (assigning probabilities to) some text. It is related to Shannon's Entropy. Lower the entropy (uncertainty), lower the perplexity. If a model, which is trained on good blogs and is being evaluated on similarly looking good blogs, assigns higher probability, we say the model has lower perplexity than a model which assigns lower probability.Originally Answered: what is perplexity in NLP?
In English, the word 'perplexed' means 'puzzled' or 'confused' (source). Perplexity means inability to deal with or understand something complicated or unaccountable.

When a toddler or a baby speaks unintelligibly, we find ourselves 'perplexed'. Why ? because their spoken language does not comply with the grammar and construct of the language that we tend to understand and speak.

Now imagine you trained a machine learning NLP model on lots and lots of well written blogs and answers. Now the task is to evaluate how good a certain Quora answer is (for, say, pushing it to the top of the feed). Among very many models that you trained, which model will you select for picking good blogs from bad ?

Answer: You would pick that NLP model which is least "perplexed" when presented with a well written blog.

Thus, perplexity metric in NLP is a way to capture the degree of 'uncertainty' a model has in predicting (assigning probabilities to) some text. It is related to Shannon's Entropy. Lower the entropy (uncertainty), lower the perplexity. If a model, which is trained on good blogs and is being evaluated on similarly looking good blogs, assigns higher probability, we say the model has lower perplexity than a model which assigns lower probability.Originally Answered: what is perplexity in NLP?
In English, the word 'perplexed' means 'puzzled' or 'confused' (source). Perplexity means inability to deal with or understand something complicated or unaccountable.

When a toddler or a baby speaks unintelligibly, we find ourselves 'perplexed'. Why ? because their spoken language does not comply with the grammar and construct of the language that we tend to understand and speak.

Now imagine you trained a machine learning NLP model on lots and lots of well written blogs and answers. Now the task is to evaluate how good a certain Quora answer is (for, say, pushing it to the top of the feed). Among very many models that you trained, which model will you select for picking good blogs from bad ?

Answer: You would pick that NLP model which is least "perplexed" when presented with a well written blog.

Thus, perplexity metric in NLP is a way to capture the degree of 'uncertainty' a model has in predicting (assigning probabilities to) some text. It is related to Shannon's Entropy. Lower the entropy (uncertainty), lower the perplexity. If a model, which is trained on good blogs and is being evaluated on similarly looking good blogs, assigns higher probability, we say the model has lower perplexity than a model which assigns lower probability.Originally Answered: what is perplexity in NLP?
In English, the word 'perplexed' means 'puzzled' or 'confused' (source). Perplexity means inability to deal with or understand something complicated or unaccountable.

When a toddler or a baby speaks unintelligibly, we find ourselves 'perplexed'. Why ? because their spoken language does not comply with the grammar and construct of the language that we tend to understand and speak.

Now imagine you trained a machine learning NLP model on lots and lots of well written blogs and answers. Now the task is to evaluate how good a certain Quora answer is (for, say, pushing it to the top of the feed). Among very many models that you trained, which model will you select for picking good blogs from bad ?

Answer: You would pick that NLP model which is least "perplexed" when presented with a well written blog.

Thus, perplexity metric in NLP is a way to capture the degree of 'uncertainty' a model has in predicting (assigning probabilities to) some text. It is related to Shannon's Entropy. Lower the entropy (uncertainty), lower the perplexity. If a model, which is trained on good blogs and is being evaluated on similarly looking good blogs, assigns higher probability, we say the model has lower perplexity than a model which assigns lower probability.Originally Answered: what is perplexity in NLP?
In English, the word 'perplexed' means 'puzzled' or 'confused' (source). Perplexity means inability to deal with or understand something complicated or unaccountable.

When a toddler or a baby speaks unintelligibly, we find ourselves 'perplexed'. Why ? because their spoken language does not comply with the grammar and construct of the language that we tend to understand and speak.

Now imagine you trained a machine learning NLP model on lots and lots of well written blogs and answers. Now the task is to evaluate how good a certain Quora answer is (for, say, pushing it to the top of the feed). Among very many models that you trained, which model will you select for picking good blogs from bad ?

Answer: You would pick that NLP model which is least "perplexed" when presented with a well written blog.

Thus, perplexity metric in NLP is a way to capture the degree of 'uncertainty' a model has in predicting (assigning probabilities to) some text. It is related to Shannon's Entropy. Lower the entropy (uncertainty), lower the perplexity. If a model, which is trained on good blogs and is being evaluated on similarly looking good blogs, assigns higher probability, we say the model has lower perplexity than a model which assigns lower probability.